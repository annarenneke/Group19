{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE ML:**\n",
    "\n",
    "**Step A: What problem?**\n",
    "    What kind of problem am I working on?\n",
    "    My target is:\n",
    "        - Contineous:\n",
    "            - We want to use a regression model\n",
    "            - We aim for a model with least MSe\n",
    "        - Categorical/finite integers:\n",
    "            - Classification model\n",
    "            - Highest accuracy (ACC)\n",
    "            \n",
    "#our answer\n",
    "We would like to predict the individual review score/rating based opon several factors such as area, price, weather and so on.\n",
    "\n",
    "Our target is categorical which can take 5 different values; 1 to 5. We will use a classification model based on that. Classification is a technique where we categorize data into a given number of classes - hence our 5 classes of ratings. The main goal of a classification problem is to identify the category/class to which a new data will fall under (https://analyticsindiamag.com/7-types-classification-algorithms/)\n",
    "\n",
    "\n",
    "Different models:\n",
    "    - Logistic Regression\n",
    "    - Na√Øve Bayes\n",
    "    - Stochastic Gradient Descent\n",
    "    - K-Nearest Neighbours\n",
    "    - Decision Tree (YES)\n",
    "    - Random Forest (YES)\n",
    "    - Support Vector Machine (YES)\n",
    "            \n",
    "**Step B: Which model?**\n",
    "    If regression:\n",
    "        - a linear model (OLS, Lasso, Ridge, Elastic net)\n",
    "      If classification:\n",
    "        - Logistic Regression (w/ or w/o regularization)\n",
    "\n",
    "**Step C:**\n",
    "    What hyperparameters exist for the model I have chosen?\n",
    "    - Ridge/Lasso: \\lambda\n",
    "    - Elastic net: \\lambda_1, \\lambda_2\n",
    "    - Logistic regression with L1 + L2\n",
    " \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML STEPS:**\n",
    "\n",
    "**Step 1: Data split**\n",
    "Split into test and development (train) data\n",
    "    - A normal split is 30% for test and 70% for train if you have X observations\n",
    "    - If you have more than 10.000 observations then use 20% for test, and 80% for train\n",
    "\n",
    "Polynomial transformation of features:\n",
    "    - It is optimal only for linear and logistic models\n",
    "\n",
    "**Step 2: Model pipeline:**\n",
    "    - Construct a model building pipeline\n",
    "    Preprocessing phase:\n",
    "    - polynomial expansion, variable scaling\n",
    "    Supervised model (classification or regression)\n",
    "        Good to use pipeline: make sure that we dont have dataleakage - meaning that you mix test and training data\n",
    "\n",
    "**Step 3: Model selection**\n",
    "- Main idea\n",
    "    - we want to select the optimal model\n",
    "    - we measure model performance with out-of-sample prediction on validation data\n",
    "  \n",
    "  Implementation:\n",
    "  - Pick the model which performed the best on the validation data during cross validation\n",
    "  - Cross validayion gives the best model performance\n",
    "  - e.g. use gridsearcg for more than 2 hyperparameters\n",
    "  \n",
    "Cross validation:\n",
    "    We only use the training/development data\n",
    "    Use 10-fold CV and split into 10 validation bins\n",
    "    For each validation bin:\n",
    "    - we fit our model on the data outside the validation bin i.e. in one of the remaining 9\n",
    "    - we transform and predict the target in the validation bin using our model\n",
    "    Note: we must perform out whole model building process and transformation in each fold\n",
    "    \n",
    " Finally we compute the mean across the 10 validation bins for each hyperparameters.\n",
    " \n",
    " *Chose the hyperparameters and estimate the model with them on the development sent\n",
    " \n",
    "** Step 4: Check list **\n",
    " Check that we have no data leakage\n",
    " - Never fit our model building on val/test data\n",
    " Ensure that model has converged\n",
    " \n",
    " \n",
    " **Step 5: Final model: training and evaluation**\n",
    " We train the model with the optimal hyperparameters on ALL the training/development data\n",
    " Evaluate the data model out-of-sample on the testset \n",
    " Use cross validation for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['target'] = DF.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(DF.drop(['target'], axis ='columns'),digtigs.target,test_size=0.2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train) #trains the model\n",
    "model.score(X_test, y_test) #the accucarcy\n",
    "\n",
    "#we tune the model:\n",
    "model = RandomForestClassifier(n_estimator=20)\n",
    "#print the acc again - you can continue\n",
    "\n",
    "y_predicted = model.predict(X_test) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "cm\n",
    "\n",
    "#the plot\n",
    "from seaborn import sn\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "\n",
    "#https://www.youtube.com/watch?v=ok2s1vV9XW0&fbclid=IwAR3ZwEq23AbA4pj4vrh69sohnBL9YPuwFN1EHUb0aQkPSsRvuQclhQoQCf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
