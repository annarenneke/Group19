{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code:\n",
    "    \n",
    "1. Install packages + read data \n",
    "2. Generate training set (split data)\n",
    "3. Generate model\n",
    "4. Train the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data collection\n",
    "2. Data preparation\n",
    "3. Choosing a model\n",
    "4. Training the model\n",
    "5. Evaluate the model\n",
    "6. Parameter tuning -> hyperparameters\n",
    "7. Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Our data:**\n",
    "   - Is it categorical or contineous?\n",
    "       * Continous data: their possible values cannot be counted and can only be described using intervals on the real number line. For example, the exact amount of gas purchased at the pump for cars with 20-gallon tanks would be continuous data from 0 gallons to 20 gallons, represented by the interval [0, 20], inclusive. You might pump 8.40 gallons, or 8.41, or 8.414863 gallons, or any possible number from 0 to 20. In this way, continuous data can be thought of as being uncountably infinite\n",
    "       * Categorical data: Categorical data represent characteristics such as a person’s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as “1” indicating male and “2” indicating female), but those numbers don’t have mathematical meaning. You couldn’t add them together, for example. Similarly movie, music and video game genres, weather, country names, food and cuisine types are other examples of nominal categorical attributes.\n",
    "       \n",
    "**2. Models:**     \n",
    "We would like to predict the restaurants rating from different factors such as; weather, location, cuisine, price, month ....\n",
    "Model: \n",
    "score = beta_0 + beta_1*price+beta_2*location + ... + error\n",
    "\n",
    "\n",
    "- Linear regression\n",
    "- Lasso\n",
    "- Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10 - Modeling and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some data $y$ we want to model/predict from input $x$.  \n",
    "\n",
    "The aim is to find a function $f$ such that the distance between actual values $y$ and predicted values $f(x)$ are minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have model $y=x^T\\beta$\n",
    "\n",
    "We distinguish by type of the `target` variable `y`:\n",
    "- **regression**: predict a numeric value\n",
    "- **classification**: distinguish between target categories (non-numeric data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "ML lingo and econometric equivalents (in italic)\n",
    "\n",
    "- `feature` vector, $\\textbf{x}_i$, i.e a row of input variables\n",
    "  - = explanatory *variables* in econometrics\n",
    "- `weight` vector, $\\textbf{w}$, i.e model parameters\n",
    "  - = *coefficients* in econometrics where denoted $\\beta$\n",
    "- `bias` term, $w_0$, i.e. the model intercept\n",
    "  - = the *constant* variable in denoted $\\beta_0$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we estimate the model parameters?\n",
    "\n",
    "Step 1. initialize the weights,  $\\hat{w}$, with small random numbers\n",
    "\n",
    "Step 2. for each (training) observation, i=1, .., n\n",
    "  1. compute predicted target, $\\hat{y}_i$\n",
    "  1. update weights $\\hat{w}$ based on perceptron rule (explanation follows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we compute the predicted target $\\hat{y}$?*\n",
    "\n",
    "We apply a transformation on the net-input :\n",
    "- single observation, expanded notation:\n",
    "\\begin{align*}\n",
    "\\hat{y}_i= \\phi(z_i),\\quad z_i=w_0+w_1x_{i,1}+...+w_kx_{i,k}\n",
    "\\end{align*}\n",
    "\n",
    "- single observation, vector notation:\n",
    "\\begin{align*}\n",
    "\\hat{y}_i= \\phi(z_i),\\quad z_i=\\boldsymbol{w}^{T}\\boldsymbol{x}_i\n",
    "\\end{align*}\n",
    "\n",
    "- multiple observations, matrix notation:\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{y}}= & \\phi(\\boldsymbol{z}),\\quad\\boldsymbol{z}=\\boldsymbol{X}\\boldsymbol{w}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Idea: Use some of our sample for model evaluation.\n",
    "- Implementation - divide data randomly into two subsets:\n",
    "    - `training data` for estimation; \n",
    "    - `test data` for evaluation.\n",
    "- Note: does not work for time series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 - Regression and regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two agendas (1)\n",
    "\n",
    "What are the objectives of empirical research? \n",
    "\n",
    "1. *causation*: what is the effect of a particular variable on an outcome? \n",
    "2. *prediction*: find some function that provides a good prediction of $y$ as a function of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two agendas (2)\n",
    "\n",
    "How might we express the agendas in a model?\n",
    "\n",
    "$$ y = \\alpha + \\beta x + \\varepsilon $$\n",
    "\n",
    "- *causation*: interested in $\\hat{\\beta}$ \n",
    "\n",
    "- *prediction*: interested in $\\hat{y}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a polynomial (1)\n",
    "Polyonomial: $f(x) = 2+8*x^4$\n",
    "\n",
    "Try models of increasing order polynomials. \n",
    "\n",
    "- Split data into train and test (50/50)\n",
    "- For polynomial order 0 to 9:\n",
    "    - Iteration n: $y = \\sum_{k=0}^{n}(\\beta_k\\cdot x^k)+\\varepsilon$. (Taylor expansion)\n",
    "    - Estimate order n model on training data\n",
    "    - Evaluate with on test data with $\\log RMSE$ ($= \\log \\sqrt{SSE/n}$)b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we regularize?\n",
    "- To mitigate overfitting > better model predictions\n",
    "\n",
    "How do we regularize?\n",
    "- We make models which are less complex:\n",
    "  - reducing the **number** of coefficient;\n",
    "  - reducing the **size** of the coefficients.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a penalty term our optimization procedure:\n",
    "    \n",
    "$$ \\text{arg min}_\\beta \\, \\underset{\\text{MSE=SSE/n}}{\\underbrace{E[(y_0 - \\hat{f}(x_0))^2]}} + \\underset{\\text{penalty}}{\\underbrace{\\lambda \\cdot R(\\beta)}}$$\n",
    "\n",
    "Introduction of penalties implies that increased model complexity has to be met with high increases precision of estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most common penalty functions are L1 and L2 regularization.\n",
    "\n",
    "- L1 regularization (***Lasso***): $R(\\beta)=\\sum_{j=1}^{p}|\\beta_j|$ \n",
    "    - Makes coefficients sparse, i.e. selects variables by removing some (if $\\lambda$ is high)\n",
    "    \n",
    "    \n",
    "- L2 regularization (***Ridge***): $R(\\beta)=\\sum_{j=1}^{p}\\beta_j^2$\n",
    "    - Reduce coefficient size\n",
    "    - Fast due to analytical solution\n",
    "    \n",
    "*To note:* The *Elastic Net* uses a combination of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 - Module selection and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) 2019:\n",
    "\n",
    "- model **bias**: _an error from erroneous assumptions in the learning algorithm_\n",
    "  - high bias can cause an algorithm to miss the relevant relations between features and target outputs (**underfitting**)\n",
    "   \n",
    "\n",
    "- model **variance**: _an error from sensitivity to small fluctuations in the training set_\n",
    "  -  high variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (**overfitting**).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is: low bias / high variance\n",
    "\n",
    "- traning our model captures all patterns but we also find some irrelevant\n",
    "- reacts too much to training sample errors \n",
    "    - some errors are just noise, and thus we find too many spurious relations \n",
    "- examples of causes: \n",
    "    - too much polynomial expansion of variables (`PolynomialFeatures`)\n",
    "    - non-linear/logistic without properly tuned hyperparameters: \n",
    "        - Decision Trees, Support Vector Machines or Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting is: high bias / low variance\n",
    "- oversimplification of models, cannot approximate all patterns found\n",
    "- examples of causes: \n",
    "    - linear and logistic regression (without polynomial expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The hold-out method:**\n",
    "We reuse the data in the development set repeatedly\n",
    "- We test on all the data\n",
    "- Rotate which parts of data is used for test and train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out CV**\n",
    "The most robust approach\n",
    "- Each single observation in the training data we use the remaining data to train.\n",
    "- Makes number of models equal to the number of observations\n",
    "- Very computing intensive - does not scale!\n",
    "LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-fold method**\n",
    "We split the sample into $K$ even sized test bins.\n",
    "- For each test bin $k$ we use the remaining data for training.\n",
    "\n",
    "Advantages:\n",
    "- We use all our data for testing.\n",
    "- Training is done with 100-(100/K) pct. of the data, i.e. 90 pct. for K=10.\n",
    "\n",
    "We compute MSE for every lambda and every fold (nested for loop)\n",
    "\n",
    "- Getting more evaluations of our model performance.\n",
    "- We can cross validate at two levels:\n",
    "    - Outer: we make multiple splits of test and train/dev.\n",
    "    - Inner: within each train/dev. dataset we make cross validation to choose hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
